{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Foundation models fine-tuned for coding purposes\n",
    "- Code llama instruct: Allows human interaction for coding.\n",
    "- Largest code llama model is 34B.\n",
    "- Code llama doesn't require instruction tags around the prompt.\n",
    "- Can accept longer input than chat models. So, you can input an entire program and ask Code Llama to review it for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinan/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import llama, code_llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing code to solve math problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of min and max temp days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_min = [42, 52, 47, 47, 53, 48, 47, 53, 55, 56, 57, 50, 48, 45]\n",
    "temp_max = [55, 57, 59, 59, 58, 62, 65, 65, 64, 63, 60, 60, 62, 62]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use a llama chat model to get the day with lowest temp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the temperature forecast you provided, the day with the lowest temperature is Day 7, with a low temperature of 47°F (8.3°C).\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Below is the 14 day temperature forecast in fahrenheit degree:\n",
    "14-day low temperatures: {temp_min}\n",
    "14-day high temperatures: {temp_max}\n",
    "Which day has the lowest temperature?\n",
    "\"\"\"\n",
    "\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's wrong. Let's ask Code Llama to write a python function to determine the minimum temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "[PYTHON]\n",
      "def get_min_max(temp_min, temp_max):\n",
      "    return min(temp_min), max(temp_max)\n",
      "[/PYTHON]\n",
      "[TESTS]\n",
      "# Test case 1:\n",
      "assert get_min_max([1, 2, 3], [4, 5, 6]) == (1, 6)\n",
      "# Test case 2:\n",
      "assert get_min_max([1, 2, 3], [4, 5, 6, 7]) == (1, 7)\n",
      "# Test case 3:\n",
      "assert get_min_max([1, 2, 3, 4], [4, 5, 6]) == (1, 6)\n",
      "[/TESTS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Write Python code that can calculate\n",
    "the minimum of the list temp_min\n",
    "and the maximum of the list temp_max\n",
    "\"\"\"\n",
    "response_2 = code_llama(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function on the temperature lists above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 65)\n"
     ]
    }
   ],
   "source": [
    "def get_min_max(temp_min, temp_max):\n",
    "    return min(temp_min), max(temp_max)\n",
    "\n",
    "temp_min = [42, 52, 47, 47, 53, 48, 47, 53, 55, 56, 57, 50, 48, 45]\n",
    "temp_max = [55, 57, 59, 59, 58, 62, 65, 65, 64, 63, 60, 60, 62, 62]\n",
    "\n",
    "results = get_min_max(temp_min, temp_max)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code in-filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Code Llama to fill in partially completed code.\n",
    "- Notice the [INST] and [/INST] tags that have been added to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "def star_rating(n):\n",
      "'''\n",
      "  This function returns a rating given the number n,\n",
      "  where n is an integer from 1 to 5.\n",
      "'''\n",
      "\n",
      "    if n == 1:\n",
      "        rating=\"poor\"\n",
      "    <FILL>\n",
      "    elif n == 5:\n",
      "        rating=\"excellent\"\n",
      "\n",
      "    return rating\n",
      "[/INST]\n",
      "\n",
      "model: togethercomputer/CodeLlama-7b-Instruct\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "def star_rating(n):\n",
    "'''\n",
    "  This function returns a rating given the number n,\n",
    "  where n is an integer from 1 to 5.\n",
    "'''\n",
    "\n",
    "    if n == 1:\n",
    "        rating=\"poor\"\n",
    "    <FILL>\n",
    "    elif n == 5:\n",
    "        rating=\"excellent\"\n",
    "\n",
    "    return rating\n",
    "\"\"\"\n",
    "\n",
    "response = code_llama(prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "[PYTHON]\n",
      "def star_rating(n):\n",
      "    if n == 1:\n",
      "        rating = \"poor\"\n",
      "    elif n == 2:\n",
      "        rating = \"fair\"\n",
      "    elif n == 3:\n",
      "        rating = \"average\"\n",
      "    elif n == 4:\n",
      "        rating = \"good\"\n",
      "    else:\n",
      "        rating = \"excellent\"\n",
      "    return rating\n",
      "[/PYTHON]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write code to calculate the nth Fibonacci number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Fibonacci sequence:\n",
    "\n",
    "`0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610...`\n",
    "\n",
    "Each number (after the starting 0 and 1) is equal to the sum of the two numbers that precede it.\n",
    "\n",
    "Write a natural language prompt that asks the model to write code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "Provide a function that calculates the n-th fibonacci number.\n",
      "[/INST]\n",
      "\n",
      "model: togethercomputer/CodeLlama-7b-Instruct\n",
      "  \n",
      "[PYTHON]\n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "[/PYTHON]\n",
      "[TESTS]\n",
      "# Test case 1:\n",
      "assert fibonacci(0) == 0\n",
      "# Test case 2:\n",
      "assert fibonacci(1) == 1\n",
      "# Test case 3:\n",
      "assert fibonacci(2) == 1\n",
      "# Test case 4:\n",
      "assert fibonacci(3) == 2\n",
      "# Test case 5:\n",
      "assert fibonacci(6) == 8\n",
      "[/TESTS]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Provide a function that calculates the n-th fibonacci number.\n",
    "\"\"\"\n",
    "\n",
    "response = code_llama(prompt, verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "    \n",
    "print(fibonacci(0))\n",
    "print(fibonacci(2))\n",
    "print(fibonacci(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! But this may not be the optimal implementation. Let's see what we can do about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the code more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "For the following code: \n",
      "def fibonacci(n):\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "Is this implementation efficient?\n",
      "Please explain.\n",
      "[/INST]\n",
      "\n",
      "model: togethercomputer/CodeLlama-7b-Instruct\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "\n",
    "prompt_1 = f\"\"\"\n",
    "For the following code: {code}\n",
    "Is this implementation efficient?\n",
    "Please explain.\n",
    "\"\"\"\n",
    "response_1 = code_llama(prompt_1, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No, this implementation is not efficient. The time complexity of this function is O(2^n), which means that the time it takes to compute the nth Fibonacci number grows exponentially with the size of the input.\n",
      "\n",
      "The reason for this is that the function calls itself recursively twice, once for each term in the Fibonacci sequence. Each recursive call requires the computation of the previous two terms, which means that the time complexity grows exponentially with the size of the input.\n",
      "\n",
      "A more efficient implementation of the Fibonacci sequence would be to use a loop instead of recursion, like this:\n",
      "```\n",
      "def fibonacci(n):\n",
      "    a, b = 0, 1\n",
      "    for i in range(n):\n",
      "        a, b = b, a + b\n",
      "    return a\n",
      "```\n",
      "This implementation has a time complexity of O(n), which means that the time it takes to compute the nth Fibonacci number grows linearly with the size of the input.\n",
      "\n",
      "In general, it's important to be aware of the time complexity of your algorithms and to choose implementations that have a lower time complexity. This can help you avoid running into performance issues when dealing with large inputs.\n"
     ]
    }
   ],
   "source": [
    "print(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the original function and the more efficient implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    else:\n",
    "        return fibonacci(n-1) + fibonacci(n-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci_fast(n):\n",
    "    a, b = 0, 1\n",
    "    for i in range(n):\n",
    "        a, b = b, a + b\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by asking Code Llama to write Python code that calculates how long a piece of code takes to execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "[INST]\n",
      "Provide sample code that calculates the runtime of a Python function call.\n",
      "[/INST]\n",
      "\n",
      "model: togethercomputer/CodeLlama-7b-Instruct\n",
      "  Here is an example of how you can calculate the runtime of a Python function call using the `time` module:\n",
      "```\n",
      "import time\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "start_time = time.time()\n",
      "my_function()\n",
      "end_time = time.time()\n",
      "\n",
      "print(\"Runtime:\", end_time - start_time)\n",
      "```\n",
      "This code will print the runtime of the `my_function()` call in seconds.\n",
      "\n",
      "Alternatively, you can use the `timeit` module to measure the runtime of a function call. Here is an example:\n",
      "```\n",
      "import timeit\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "print(timeit.timeit(my_function))\n",
      "```\n",
      "This code will print the runtime of the `my_function()` call in seconds.\n",
      "\n",
      "You can also use the `cProfile` module to profile the runtime of a function call. Here is an example:\n",
      "```\n",
      "import cProfile\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "cProfile.run('my_function()')\n",
      "```\n",
      "This code will print the runtime of the `my_function()` call in seconds, as well as other information such as the number of calls and the total time spent in the function.\n",
      "\n",
      "You can also use the `profile` module to profile the runtime of a function call. Here is an example:\n",
      "```\n",
      "import profile\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "profile.run('my_function()')\n",
      "```\n",
      "This code will print the runtime of the `my_function()` call in seconds, as well as other information such as the number of calls and the total time spent in the function.\n",
      "\n",
      "You can also use the `line_profiler` module to profile the runtime of a function call. Here is an example:\n",
      "```\n",
      "import line_profiler\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "line_profiler.run('my_function()')\n",
      "```\n",
      "This code will print the runtime of the `my_function()` call in seconds, as well as other information such as the number of calls and the total time spent in the function.\n",
      "\n",
      "You can also use the `memory_profiler` module to profile the memory usage of a function call. Here is an example:\n",
      "```\n",
      "import memory_profiler\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "memory_profiler.run('my_function()')\n",
      "```\n",
      "This code will print the memory usage of the `my_function()` call in bytes, as well as other information such as the number of calls and the total memory usage.\n",
      "\n",
      "You can also use the `psutil` module to profile the memory usage of a function call. Here is an example:\n",
      "```\n",
      "import psutil\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "psutil.memory_usage(my_function)\n",
      "```\n",
      "This code will print the memory usage of the `my_function()` call in bytes, as well as other information such as the number of calls and the total memory usage.\n",
      "\n",
      "You can also use the `pympler` module to profile the memory usage of a function call. Here is an example:\n",
      "```\n",
      "import pympler\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "pympler.memory_usage(my_function)\n",
      "```\n",
      "This code will print the memory usage of the `my_function()` call in bytes, as well as other information such as the number of calls and the total memory usage.\n",
      "\n",
      "You can also use the `memory_profiler` module to profile the memory usage of a function call. Here is an example:\n",
      "```\n",
      "import memory_profiler\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "memory_profiler.run('my_function()')\n",
      "```\n",
      "This code will print the memory usage of the `my_function()` call in bytes, as well as other information such as the number of calls and the total memory usage.\n",
      "\n",
      "You can also use the `psutil` module to profile the memory usage of a function call. Here is an example:\n",
      "```\n",
      "import psutil\n",
      "\n",
      "def my_function():\n",
      "    # do something\n",
      "    pass\n",
      "\n",
      "psutil.memory_usage(my_function)\n",
      "```\n",
      "This code will print the memory usage of the `my_function()` call in bytes, as well as other information such as the number of calls and the total memory usage.\n",
      "\n",
      "You can also use the `pympler` module to profile the memory usage of a\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Provide sample code that calculates the runtime \\\n",
    "of a Python function call.\n",
    "\"\"\"\n",
    "\n",
    "response = code_llama(prompt, verbose=True)\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "n=40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recursive fibonacci(40) \n",
      "runtime in seconds: 106.58943891525269\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "fibonacci(n) # note, we recommend keeping this number <=40\n",
    "end_time = time.time()\n",
    "print(f\"recursive fibonacci({n}) \")\n",
    "print(f\"runtime in seconds: {end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-recursive fibonacci(40) \n",
      "runtime in seconds: 8.58306884765625e-05\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "fibonacci_fast(n) # note, we recommend keeping this number <=40\n",
    "end_time = time.time()\n",
    "print(f\"non-recursive fibonacci({n}) \")\n",
    "print(f\"runtime in seconds: {end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code llama can take longer texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code Llama models can handle much larger input text than the Llama Chat models - more than 20,000 characters.\n",
    "- The size of the input text is known as the context window.\n",
    "- The following code will return an error because the sum of the input and output tokens is larger than the limit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4097. Given: 3973 `inputs` tokens and 1024 `max_new_tokens`', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "with open(\"TheVelveteenRabbit.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "prompt=f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n \n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Ask the 7B model to respond\n",
    "response = llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Response from the code llama model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The little Rabbit was thinking about the strange rabbits he had\n",
      "seen in the wood, and how they had run away from him. He was thinking\n",
      "about how he had been so lonely and how he had longed for someone to\n",
      "talk to, and how he had been so happy when the Boy had come and\n",
      "carried him home. He was thinking about how the Boy had said he was\n",
      "Real, and how he had been so proud of that. And he was thinking about\n",
      "how he had grown old and shabby, and how he had lost his shape, and\n",
      "how he had been so happy when the Boy had been ill and needed him.\n",
      "\n",
      "And then, all at once, he remembered something. He remembered that\n",
      "the Boy had said he was Real, and that he had been so proud of that.\n",
      "And he remembered that he had been so lonely, and that he had longed\n",
      "for someone to talk to. And he remembered that he had been so happy\n",
      "when the Boy had come and carried him home. And he remembered that he\n",
      "had grown old and shabby, and that he had lost his shape. And he\n",
      "remembered that he was Real, and that he was loved by the Boy.\n",
      "\n",
      "And so he sat up straight and looked at the Boy, and he knew that he\n",
      "was Real, and that he was loved by the Boy. And he knew that he was\n",
      "happy, and that he was loved, and that he was Real. And he knew that\n",
      "he was a toy no longer, but a Real rabbit, and that he was loved by\n",
      "the Boy. And he knew that he was happy, and that he was loved, and\n",
      "that he was Real.\n"
     ]
    }
   ],
   "source": [
    "from utils import llama\n",
    "with open(\"TheVelveteenRabbit.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "prompt=f\"\"\"\n",
    "Give me a summary of the following text in 50 words:\\n\\n \n",
    "{text}\n",
    "\"\"\"\n",
    "response = code_llama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts on Code Llama's summarization performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the Code Llama model could handle the longer text, the output here isn't that great - the response is very repetitive.\n",
    "\n",
    "- Code Llama's primary skill is writing code.\n",
    "- Experiment to see if you can prompt the Code Llama model to improve its output.\n",
    "- You may need to trade off performance and input text size depending on your task.\n",
    "- You could ask Llama 2 70B chat to help you evaluate how well the Code Llama model is doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
